{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Java OpenMP Patternlets",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rkurniawati/pyjama-patternlets/blob/master/Java_OpenMP_Patternlets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIYdn1woOS1n"
      },
      "source": [
        "#Java OpenMP Patternlet Notebook\n",
        "\n",
        "Adapted to Java by Ruth Kurniawati (Westfield State University) based on the [PDC book](https://pdcbook.calvin.edu/pdcbook/RaspberryPiHandout/) from [CSInParallel](https://csinparallel.org/index.html). \n",
        "\n",
        "This notebook contains OpenMP patternlet examples in the Java programming language. Patterns are reusable solution for commonly occuring problems. The OpenMP patternlets are reusable solution that were originally written in the C language with the OpenMP library by Joel Adams: \n",
        "\n",
        "Adams, Joel C. \"Patternlets: A Teaching Tool for Introducing Students to Parallel Design Patterns.\" 2015 IEEE International Parallel and Distributed Processing Symposium Workshop. IEEE, 2015.\n",
        "\n",
        "However, OpenMP library is only available for C/C++ and Fortran languages. For Java, Pyjama provides support for OpenMP-like directive. More information about Pyjama can be found in the paper below:\n",
        "\n",
        "Vikas, Nasser Giacaman, and Oliver Sinnen. 2013. Pyjama: OpenMP-like implementation for Java, with GUI extensions. In <i>Proceedings of the 2013 International Workshop on Programming Models and Applications for Multicores and Manycores</i> (<i>PMAM '13</i>). Association for Computing Machinery, New York, NY, USA, 43–52. DOI:https://doi.org/10.1145/2442992.2442997"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MmtgBkPiZrw"
      },
      "source": [
        "# Multicore Systems and Multi-Threading\n",
        "\n",
        "Before proceeding with the examples, let's investigate the computer that this notebook is running on. For this, let's use the `lscpu` command. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khorTJcRlozK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8af471e-5c57-40b1-fa67-fbde28d03f2b"
      },
      "source": [
        "!lscpu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Architecture:        x86_64\n",
            "CPU op-mode(s):      32-bit, 64-bit\n",
            "Byte Order:          Little Endian\n",
            "CPU(s):              2\n",
            "On-line CPU(s) list: 0,1\n",
            "Thread(s) per core:  2\n",
            "Core(s) per socket:  1\n",
            "Socket(s):           1\n",
            "NUMA node(s):        1\n",
            "Vendor ID:           GenuineIntel\n",
            "CPU family:          6\n",
            "Model:               63\n",
            "Model name:          Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "Stepping:            0\n",
            "CPU MHz:             2299.998\n",
            "BogoMIPS:            4599.99\n",
            "Hypervisor vendor:   KVM\n",
            "Virtualization type: full\n",
            "L1d cache:           32K\n",
            "L1i cache:           32K\n",
            "L2 cache:            256K\n",
            "L3 cache:            46080K\n",
            "NUMA node0 CPU(s):   0,1\n",
            "Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt arat md_clear arch_capabilities\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY1glALCX8qK"
      },
      "source": [
        "## Cores, Processes and Threads\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo98NtHglr9y"
      },
      "source": [
        "If you run `lscpu` in the notebook, you may see output similar to below:\n",
        "\n",
        "```\n",
        "Architecture:        x86_64\n",
        "CPU op-mode(s):      32-bit, 64-bit\n",
        "Byte Order:          Little Endian\n",
        "CPU(s):              2\n",
        "On-line CPU(s) list: 0,1\n",
        "Thread(s) per core:  2\n",
        "Core(s) per socket:  1\n",
        "Socket(s):           1\n",
        "NUMA node(s):        1\n",
        "Vendor ID:           GenuineIntel\n",
        "CPU family:          6\n",
        "Model:               79\n",
        "Model name:          Intel(R) Xeon(R) CPU @ 2.20GHz\n",
        "Stepping:            0\n",
        "CPU MHz:             2199.998\n",
        "BogoMIPS:            4399.99\n",
        "Hypervisor vendor:   KVM\n",
        "Virtualization type: full\n",
        "L1d cache:           32K\n",
        "L1i cache:           32K\n",
        "L2 cache:            256K\n",
        "L3 cache:            56320K\n",
        "NUMA node0 CPU(s):   0,1\n",
        "Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
        "```\n",
        "\n",
        "This output means that the computer has 2 CPUs -- however, there is actually only one physical core but this core can execute 2 execution threads. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdhYtKRaYTBj"
      },
      "source": [
        "The `lscpu` command tells us a LOT of useful information, including the number of available cores. In this case we know that there is one socket (or chip) with 1 physical core, where each core can support 2 thread. On larger systems, it is common to see multiple threads supported per core. This is an example of **simultaneous multi-threading** (SMT, or Hyperthreading on Intel systems). A **core** can be thought of as the compute unit of the CPU. It includes registers, an ALU, and control units.\n",
        "\n",
        "Before we can discuss what a thread is, we must first discuss what a process is. A **process** can be thought of as an abstraction of a running program. When you type a command into the command line and press Enter, the Bash shell launches a process associated with that program executable. Each process contains a copy of the code and data of the program executable, and its own allocation of the stack and heap.\n",
        "\n",
        "A **thread** is a light-weight process. While each thread gets its own stack allocation, it shares the heap, code and data of the parent process. As a result, all the threads in a multi-threaded process can access a common pool of memory. This is why multi-threading is commonly referred to as shared memory programming. A single-threaded process is also referred to as a serial process or program."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4BT6lHhc3Dv"
      },
      "source": [
        "### Process Execution\n",
        "A multicore CPU allows multiple processes to execute simultaneously, or in **parallel**. While the terms concurrency and parallel are related, it is useful to think of concurrency as a software/OS-level concept, while parallel as a hardware/execution concept. A multi-threaded program, while capable of parallel execution, runs concurrently on a system with only a single CPU core."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaLDQy2CdIB1"
      },
      "source": [
        "## Thread Execution\n",
        "\n",
        "The primary goal of creating multi-threaded programs is to decrease the speed of a program’s execution. In a program that is perfectly parallelizable (that is, all components are paralleizable), it is usually possible to distribute the work associated with a program equally among all the threads. For a program _p_ whose work is equally distributed among _t_ threads, it will take roughly _p_/_t_ time, if executed on _t_ cores.\n",
        "\n",
        "For example, if a multi-threaded process that is perfectly parallelized takes 100 seconds to execute on one core, on a multi-core system with 4 cores the program will take approximately 100/4 to execute."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iskUMiEdtbk"
      },
      "source": [
        "## Leveraging Multiple Cores\n",
        "While multicore processors are ubiquitous in today’s world, most of the popular programming languages were designed to support single-thread execution. However, several native libraries are available for supporting multi-threading in popular languages like C/C++ and FORTRAN.\n",
        "\n",
        "One of these libraries is the Open MultiProcessing (OpenMP), a popular API for shared memory programming, and a standard since 1997. A key benefit of OpenMP over explicit threading libraries like POSIX threads is the ability to incrementally add parallelism to a program. For standard threaded programs, it is usually necessary to write a lot of extra code to add multi-threading to a program. Instead, OpenMP employs a series of pragmas, or special compiler directives, that tell the compiler how to parallelize the code.\n",
        "\n",
        "OpenMP library is only available for C/C++ and Fortran languages. For Java, Pyjama compiler and runtime provide support for OpenMP-like directive. More information about Pyjama can be found in the paper below:\n",
        "\n",
        "Vikas, Nasser Giacaman, and Oliver Sinnen. 2013. Pyjama: OpenMP-like implementation for Java, with GUI extensions. In <i>Proceedings of the 2013 International Workshop on Programming Models and Applications for Multicores and Manycores</i> (<i>PMAM '13</i>). Association for Computing Machinery, New York, NY, USA, 43–52. DOI:https://doi.org/10.1145/2442992.2442997\n",
        "\n",
        "In the rest of this notebook, we will look at Java programs that use OpenMP-like directives provided by the Pyjama to explore small patterns (_patternlets_) in parallel programming. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cF_wyE-XOR6"
      },
      "source": [
        "# Pyjama OpenMP library\n",
        "\n",
        "Before proceeding to the code sample, we need to make Pyjama compiler and runtime available to this notebook. \n",
        "\n",
        "The Pyjama library used in this notebook is obtained from the [CDER project](https://tcpp.cs.gsu.edu/curriculum/?q=node/21183), specifically [the version with additional bug fixes provided by Tennnessee Tech](https://www.csc.tntech.edu/pdcincs/index.php/installation). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rn4GBwkFqmIS"
      },
      "source": [
        "## Pyjama setup\n",
        "\n",
        "First, let's download and setup Pyjama Java source code compiler and runtime library. The commands below will download a ZIP file from Tennessee Tech, unzip it, and create the Pyjama/Pyjama.jar link to point to the specific jar file extracted from the ZIP file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHHuztRRq05q",
        "outputId": "a00bbcb0-78dc-41b0-ff07-4ce90e4a3e91"
      },
      "source": [
        "!wget -O Pyjama.zip https://www.csc.tntech.edu/pdcincs/resources/modules/tools/updated/Pyjama.zip\n",
        "!unzip Pyjama.zip\n",
        "!ln Pyjama/Pyjama-3.1.0.jar Pyjama/Pyjama.jar"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-12 03:20:23--  https://www.csc.tntech.edu/pdcincs/resources/modules/tools/updated/Pyjama.zip\n",
            "Resolving www.csc.tntech.edu (www.csc.tntech.edu)... 149.149.134.5\n",
            "Connecting to www.csc.tntech.edu (www.csc.tntech.edu)|149.149.134.5|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 688047 (672K) [application/zip]\n",
            "Saving to: ‘Pyjama.zip’\n",
            "\n",
            "Pyjama.zip          100%[===================>] 671.92K  1.82MB/s    in 0.4s    \n",
            "\n",
            "2021-08-12 03:20:24 (1.82 MB/s) - ‘Pyjama.zip’ saved [688047/688047]\n",
            "\n",
            "Archive:  Pyjama.zip\n",
            "   creating: Pyjama/\n",
            "  inflating: Pyjama/Pyjama-3.1.0.jar  \n",
            "  inflating: Pyjama/set_pyjama.bat   \n",
            " extracting: Pyjama/set_pyjama.sh    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6wdCiatXgDP"
      },
      "source": [
        "## Hello world\n",
        "\n",
        "In this example, we will verify that the Pyjama installation is working and able to create multiple threads as specified in the `#omp parallel num_threads` directive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNiwv0NofcOe",
        "outputId": "1ae328b5-9204-4806-9a94-9d6748ebba1b"
      },
      "source": [
        "%%writefile HelloWorld.java\n",
        "public class HelloWorld\n",
        "{\n",
        "\t\n",
        "\tpublic static void main(String[] args) \n",
        "\t{\n",
        " \n",
        "    Pyjama.omp_set_num_threads(10);\n",
        "\t\t//#omp parallel\n",
        "\t\t{\n",
        "\t\t\tint id = Pyjama.omp_get_thread_num();\n",
        "      int numThreads = Pyjama.omp_get_num_threads();\n",
        "      System.out.print(\"Hello from thread \" + id+ \", \");\n",
        "      System.out.println(\"of a total of \"+ numThreads+ \" threads.\");\n",
        "\t\t}\n",
        "\t}\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting HelloWorld.java\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGagshBWfvIV"
      },
      "source": [
        "Note that the OpenMP directive is specified inside a single line comment that starts with `//`. For the directive to be recognized by Pyjama compiler, the `//` has to be followed immediately by `#omp`. \n",
        "\n",
        "First, let's use the Pyjama compiler to process the `#omp parallel` directive in the program."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNQrDeHAgOT4",
        "outputId": "5deb51e0-9eb5-4b81-ecb4-f4a40bdb5562"
      },
      "source": [
        "!java -jar Pyjama/Pyjama.jar HelloWorld.java"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pyjama Compiler Version: 3.1.0\n",
            "-----------------------------------------------------\n",
            "2021/08/11\t20:09:54\n",
            "-----------------------------------------------------\n",
            "Processing file: HelloWorld.java\n",
            "-----------------------------------------------------\n",
            "Processing 1st Phase: Parse and Normalisation\n",
            "Processing 2nd Phase: Symbol scoping visiting\n",
            "Processing 3rd Phase: Pyjama code translation visiting\n",
            "Processing 4th Phase: Generating java code\n",
            "Paralleled .class file is generated.\n",
            "Processing Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InLU3iqVgd2t"
      },
      "source": [
        "Now, we're ready to run the HelloWorld program. To do this, you will need to make the Pyjama.jar available in the classpath so that the Pyjama OpenMP-like runtime library is available to the HelloWorld program."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQHv7NowgkCV",
        "outputId": "1bbc658e-671c-42d9-a1b0-6f0c3c03495a"
      },
      "source": [
        "!java -cp Pyjama/Pyjama.jar:. HelloWorld"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello from thread 7, Hello from thread 6, of a total of 10 threads.\n",
            "of a total of 10 threads.\n",
            "Hello from thread 4, Hello from thread 3, Hello from thread 0, of a total of 10 threads.\n",
            "of a total of 10 threads.\n",
            "of a total of 10 threads.\n",
            "Hello from thread 5, of a total of 10 threads.\n",
            "Hello from thread 2, Hello from thread 8, of a total of 10 threads.\n",
            "Hello from thread 9, of a total of 10 threads.\n",
            "Hello from thread 1, of a total of 10 threads.\n",
            "of a total of 10 threads.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnQHI_TWgrWL"
      },
      "source": [
        "You should expect 10 lines of `Hello from thread x, of a total of y threads`. The lines may be interspersed with one another if you have more than one processors running the code. An example output is below.\n",
        "\n",
        "```\n",
        "Hello from thread 9, Hello from thread 5, Hello from thread 8, Hello from thread 2, of a total of 10 threads.\n",
        "of a total of 10 threads.\n",
        "Hello from thread 6, Hello from thread 3, of a total of 10 threads.\n",
        "Hello from thread 4, of a total of 10 threads.\n",
        "of a total of 10 threads.\n",
        "of a total of 10 threads.\n",
        "Hello from thread 7, of a total of 10 threads.\n",
        "Hello from thread 0, of a total of 10 threads.\n",
        "Hello from thread 1, of a total of 10 threads.\n",
        "of a total of 10 threads.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHnm-q8fp7ta"
      },
      "source": [
        "# A Simple Parallel Program\n",
        "\n",
        "## The SPMD Patternlet\n",
        "\n",
        "\n",
        "A patternlet is a small program that succinctly illustrates common patterns in parallel programming. The first patternlet we will study is Single Program, Multiple Data (SPMD). Let’s start by examining Spmd2.java, a program that uses OpenMP pragmas to make it easy to run a portion of the program on multiple threads. Note that the variables `id`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7xCZbDnqVAC",
        "outputId": "a9c0c6ca-15f5-45d7-b073-db2a26d3fc34"
      },
      "source": [
        "%%writefile Spmd2.java\n",
        "class Spmd2 {\n",
        "    public static void main(String[] args) {\n",
        "        if (args.length >= 1) {\n",
        "            Pyjama.omp_set_num_threads(Integer.parseInt(args[0]));\n",
        "        }\n",
        "        System.out.println();\n",
        "\n",
        "        int id, numThreads;\n",
        "        //#omp parallel shared(id, numThreads)\n",
        "        {\n",
        "            numThreads = Pyjama.omp_get_num_threads();\n",
        "            id = Pyjama.omp_get_thread_num();\n",
        "            System.out.println(\"Hello from thread \"+ id +\" of \" + numThreads);\n",
        "        }\n",
        "\n",
        "        System.out.println();\n",
        "    }\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting Spmd2.java\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blePwuMok2OP"
      },
      "source": [
        "The `omp parallel` directive tells the Pyjama compiler that the block of code within the curly braces be run on separate threads. Prior to the line with the directive, the program is run serially. When block of code marked with the `omp parallel` directive executes, Pyjama generates a a team of threads (known as forking). Each thread is assigned its own id and runs separate copies of the code between the curly braces. At the end of the block scope, Pyjama combines all the threads together to a single-threaded process (known as joining). Conceptually, the process looks like the following."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1Tpt04ZrOQ6"
      },
      "source": [
        "<img src=https://pdcbook.calvin.edu/pdcbook/RaspberryPiHandout/_images/ForkJoin_SPMD.png >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJr5ChxylkDw"
      },
      "source": [
        "## Running the Program"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMm8AUZssydy"
      },
      "source": [
        "Just like in the HelloWorld example, first we need to use the Pyjama compiler to process the `#omp` directive. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBferIsmrIRC",
        "outputId": "44fc2657-e2af-4b3c-b05a-2551003f5fce"
      },
      "source": [
        "!java -jar Pyjama/Pyjama.jar Spmd2.java"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pyjama Compiler Version: 3.1.0\n",
            "-----------------------------------------------------\n",
            "2021/08/11\t18:55:15\n",
            "-----------------------------------------------------\n",
            "Processing file: Spmd2.java\n",
            "-----------------------------------------------------\n",
            "Processing 1st Phase: Parse and Normalisation\n",
            "Processing 2nd Phase: Symbol scoping visiting\n",
            "Processing 3rd Phase: Pyjama code translation visiting\n",
            "Processing 4th Phase: Generating java code\n",
            "Paralleled .class file is generated.\n",
            "Processing Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc0C9Q-Lsrg6"
      },
      "source": [
        "Now, we're ready to run the Spmd2 program. Let's specify that you'd like to have 10 threads by supplying this number in the command line argument. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOLa2FNvrNjK",
        "outputId": "625b0349-5ede-41c8-b229-c91e319fcf0c"
      },
      "source": [
        "!java -cp Pyjama/Pyjama.jar:. Spmd2 10 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Hello from thread 0 of 10\n",
            "Hello from thread 1 of 10\n",
            "Hello from thread 2 of 10\n",
            "Hello from thread 3 of 10\n",
            "Hello from thread 4 of 10\n",
            "Hello from thread 5 of 10\n",
            "Hello from thread 6 of 10\n",
            "Hello from thread 7 of 10\n",
            "Hello from thread 8 of 10\n",
            "Hello from thread 9 of 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ORBtUPclotz"
      },
      "source": [
        "Try running the program a few times with 10 threads (press the run button in the cell above). Observe the output. Occasionally something will be amiss. Do you notice it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1hj5AT8l4jg"
      },
      "source": [
        "## Race Conditions\n",
        "\n",
        "Watch this [video](\n",
        "https://d32ogoqmya1dw8.cloudfront.net/files/csinparallel/raceconditions_workshop.mov) to help you understand what's going on. Note that the video is made for the C++ version of the program, however the underlying issue is the same. \n",
        "\n",
        "The Spmd2 program has a race condition where there are more than one threads modifying a shared variable. Which shared variable(s) is/are causing the problem?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KexJJTwunv0T"
      },
      "source": [
        "## Fixing the code\n",
        "\n",
        "For this example, the race condition can be avoided by ensuring that each threads has its own copy of `id` and `numThreads` variables. Instead of declaring them as `shared` in the `omp parallel` directive, use the `private` clause as shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFBEy0tppsYa",
        "outputId": "667aee78-965e-412c-add0-b408369b723f"
      },
      "source": [
        "%%writefile Spmd2.java\n",
        "class Spmd2 {\n",
        "    public static void main(String[] args) {\n",
        "        if (args.length >= 1) {\n",
        "            Pyjama.omp_set_num_threads(Integer.parseInt(args[0]));\n",
        "        }\n",
        "        System.out.println();\n",
        "\n",
        "        int id, numThreads;\n",
        "        //#omp parallel private(id, numThreads)\n",
        "        {\n",
        "            numThreads = Pyjama.omp_get_num_threads();\n",
        "            id = Pyjama.omp_get_thread_num();\n",
        "            System.out.println(\"Hello from thread \"+ id +\" of \" + numThreads);\n",
        "        }\n",
        "\n",
        "        System.out.println();\n",
        "    }\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting Spmd2.java\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWHF-hfyp3S9"
      },
      "source": [
        "Let's compile and run this modified program."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-pLKA_Hp6GP",
        "outputId": "8ebf41fb-f4a5-4d53-9e1f-bd94d33a0982"
      },
      "source": [
        "!java -jar Pyjama/Pyjama.jar Spmd2.java"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pyjama Compiler Version: 3.1.0\n",
            "-----------------------------------------------------\n",
            "2021/08/11\t19:19:29\n",
            "-----------------------------------------------------\n",
            "Processing file: Spmd2.java\n",
            "-----------------------------------------------------\n",
            "Processing 1st Phase: Parse and Normalisation\n",
            "Processing 2nd Phase: Symbol scoping visiting\n",
            "Processing 3rd Phase: Pyjama code translation visiting\n",
            "Processing 4th Phase: Generating java code\n",
            "Paralleled .class file is generated.\n",
            "Processing Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnwWKH6hp87h",
        "outputId": "cf7dbee1-a8a2-4df9-d2f8-c7e14f365dfd"
      },
      "source": [
        "!java -cp Pyjama/Pyjama.jar:. Spmd2 10 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Hello from thread 7 of 10\n",
            "Hello from thread 4 of 10\n",
            "Hello from thread 2 of 10\n",
            "Hello from thread 9 of 10\n",
            "Hello from thread 8 of 10\n",
            "Hello from thread 0 of 10\n",
            "Hello from thread 5 of 10\n",
            "Hello from thread 1 of 10\n",
            "Hello from thread 3 of 10\n",
            "Hello from thread 6 of 10\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6-zqcZlqACB"
      },
      "source": [
        "Were you able to reproduce the race condition using the corrected program? Why should you also declare `numThreads` as a private variable?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-qDxBrjqVqA"
      },
      "source": [
        "# Running Loops in Parallel\n",
        "\n",
        "Next we will consider a program that has a loop in it. An iterative for loop is a remarkably common pattern in all programming, primarily used to perform a calculation N times, often over a set of data containing N elements, using each element in turn inside the for loop.\n",
        "\n",
        "If there are no dependencies between the iterations (i.e. the order of them is not important), then the code inside the loop can be split between forked threads. However, the programmer must first decide how to partition the work between the threads. Specifically, how many and which iterations of the loop will each thread complete on its own?\n",
        "\n",
        "The **data decomposition** pattern describes the way how work is distributed across multiple threads. This chapter presents two patternlets, parallelLoop-equalChunks and parallelLoop-chunksOf1, that describe two common data decomposition strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bpf2GVFqrFk"
      },
      "source": [
        "## Parallel Loop, Equal Chunks\n",
        "\n",
        "Let's experiment with another OpenMP directive that will divide the work in a loop into equal chunks. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UQ7T_Nlq5ZS",
        "outputId": "67fae541-5323-4e4c-a83b-4ca4bbc144a8"
      },
      "source": [
        "%%writefile ParallelLoopEqualChunks.java\n",
        "class ParallelLoopEqualChunks {\n",
        "    final static int REPS = 16;\n",
        "    public static void main(String[] args) {\n",
        "        if (args.length >= 1) {\n",
        "            Pyjama.omp_set_num_threads(Integer.parseInt(args[0]));\n",
        "        }\n",
        "        System.out.println();\n",
        "\n",
        "        //#omp parallel for  \n",
        "        for (int i = 0; i < REPS; i++) {\n",
        "            int id = Pyjama.omp_get_thread_num();\n",
        "            System.out.println(\"Thread \"+id+\" performed iteration \"+i);\n",
        "        }\n",
        "\n",
        "        System.out.println();\n",
        "    }\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing ParallelLoopEqualChunks.java\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADAdiP5UrDXi"
      },
      "source": [
        "The `omp parallel for` directive tells the Pyjama OpenMP compiler to do the following:\n",
        "- Generate a team of threads (default is equal to the number of cores)\n",
        "- Assign each thread an equal number of iterations (a chunk) of the for loop.\n",
        "- At the end of the scope of the for loop, join all the theads back to a single-threaded process.\n",
        "\n",
        "As in our previous example, the code up to the `omp parallel for` directive is run serially. The code that is in the scope of the `omp parallel for` directive (everything inside the for loop) is run in parallel, with a subset of iterations assigned to each thread. After the implicit join at the end of the for loop, the program once again is a single-threaded process that executes serially to completion.\n",
        "\n",
        "In the above program, REPS is set to 16. If the program is run with 4 threads, then each thread gets 4 iterations of the loop (see illustration below):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz0dEXBPrhCg"
      },
      "source": [
        "<img src=\"https://pdcbook.calvin.edu/pdcbook/RaspberryPiHandout/_images/ParallelFor_Chunks-4_threads-1.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUAmJL3KrmPh"
      },
      "source": [
        "## Try It Out\n",
        "\n",
        "Try compile and run the program using the following commands below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drSrg4qirvUz",
        "outputId": "666c2f88-36cb-4f00-874d-bbd3b24d8950"
      },
      "source": [
        "!java -jar Pyjama/Pyjama.jar ParallelLoopEqualChunks.java"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pyjama Compiler Version: 3.1.0\n",
            "-----------------------------------------------------\n",
            "2021/08/11\t19:27:40\n",
            "-----------------------------------------------------\n",
            "Processing file: ParallelLoopEqualChunks.java\n",
            "-----------------------------------------------------\n",
            "Processing 1st Phase: Parse and Normalisation\n",
            "Processing 2nd Phase: Symbol scoping visiting\n",
            "Processing 3rd Phase: Pyjama code translation visiting\n",
            "Processing 4th Phase: Generating java code\n",
            "Paralleled .class file is generated.\n",
            "Processing Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TuG5KOUr07X",
        "outputId": "99eed91a-87aa-4ec0-bafb-296bec4673d9"
      },
      "source": [
        "!java -cp Pyjama/Pyjama.jar:. ParallelLoopEqualChunks 4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Thread 2 performed iteration 8\n",
            "Thread 2 performed iteration 9\n",
            "Thread 2 performed iteration 10\n",
            "Thread 2 performed iteration 11\n",
            "Thread 3 performed iteration 12\n",
            "Thread 1 performed iteration 4\n",
            "Thread 1 performed iteration 5\n",
            "Thread 3 performed iteration 13\n",
            "Thread 3 performed iteration 14\n",
            "Thread 3 performed iteration 15\n",
            "Thread 0 performed iteration 0\n",
            "Thread 1 performed iteration 6\n",
            "Thread 1 performed iteration 7\n",
            "Thread 0 performed iteration 1\n",
            "Thread 0 performed iteration 2\n",
            "Thread 0 performed iteration 3\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZifCPWfEr-Vl"
      },
      "source": [
        "Try running the program a few times with 4 threads. How does the work in the for loop get assigned to the threads?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rrR9m6esSun"
      },
      "source": [
        "### Unequal Iterrations\n",
        "\n",
        "Also try using a different number of threads. Pick a number so that the number iterations cannot be equally divided by the number of threads, such as 5. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmcjcOXEsl8H",
        "outputId": "5d524ae2-39a4-4c9a-ce7e-e9bd606e2512"
      },
      "source": [
        "!java -cp Pyjama/Pyjama.jar:. ParallelLoopEqualChunks 5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Thread 2 performed iteration 7\n",
            "Thread 2 performed iteration 8\n",
            "Thread 2 performed iteration 9\n",
            "Thread 0 performed iteration 0\n",
            "Thread 0 performed iteration 1\n",
            "Thread 1 performed iteration 4\n",
            "Thread 0 performed iteration 2\n",
            "Thread 0 performed iteration 3\n",
            "Thread 3 performed iteration 10\n",
            "Thread 3 performed iteration 11\n",
            "Thread 3 performed iteration 12\n",
            "Thread 1 performed iteration 5\n",
            "Thread 1 performed iteration 6\n",
            "Thread 4 performed iteration 13\n",
            "Thread 4 performed iteration 14\n",
            "Thread 4 performed iteration 15\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWgqjv6HsuTZ"
      },
      "source": [
        "What happens to the extra iterations?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmO_5IZxs1R5"
      },
      "source": [
        "This equal-chunk decomposition is especially useful in the following scenarios:\n",
        "- Each iteration of the loop takes the same amount of time to finish\n",
        "- The loop involves accesses to data in consecutive memory locations (e.g. an array), allowing the program to take advantage of spatial locality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfmlAD0ntBTy"
      },
      "source": [
        "## Parallel Loop, Chunks of 1\n",
        "\n",
        "In some cases, it makes sense to have iterations assigned to threads in “round-robin” style. In other words, iteration 0 goes to thread 0, iteration 1 goes to thread 1, iteration 2 goes to thread 2, and so on.\n",
        "\n",
        "Let's examine the code below that directs Pyjama to do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92YGOmQOtStE",
        "outputId": "a167ccc7-c3f5-49d1-b4d4-601394f13a70"
      },
      "source": [
        "%%writefile ParallelLoopChunksOf1.java\n",
        "class ParallelLoopChunksOf1 {\n",
        "    final static int REPS = 16;\n",
        "    public static void main(String[] args) {\n",
        "        if (args.length >= 1) {\n",
        "            Pyjama.omp_set_num_threads(Integer.parseInt(args[0]));\n",
        "        }\n",
        "        System.out.println();\n",
        "\n",
        "        //#omp parallel for schedule(static,1)\n",
        "        for (int i = 0; i < REPS; i++) {\n",
        "            int id = Pyjama.omp_get_thread_num();\n",
        "            System.out.println(\"Thread \"+id+\" performed iteration \"+i);\n",
        "        }\n",
        "\n",
        "        System.out.println();\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing ParallelLoopChunksOf1.java\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akcD6-Qqtfcn"
      },
      "source": [
        "The code is nearly identical to the previous program. The difference is in the `omp` directive. The `omp parallel for` directive has a new `schedule` clause which specifies the way iterations should be assigned to threads. The `static` keyword indicates that the the compiler should assign work to each thread at compile time (a **static** scheduling policy). The `1` indicates that the chunk size should be 1 iteration. Therefore, the above code would have 16 total chunks.\n",
        "\n",
        "In the case where the number of chunks exceed the number of theads, each successive chunk is assigned to a thread in round-robin fashion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tORVRFXutPEv"
      },
      "source": [
        "### Try It Out\n",
        "\n",
        "Let's compile and run the code. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmBOb2wQt2BI",
        "outputId": "9226dd32-11fd-4102-e31a-abcd2b4da1ee"
      },
      "source": [
        "!java -jar Pyjama/Pyjama.jar ParallelLoopChunksOf1.java"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pyjama Compiler Version: 3.1.0\n",
            "-----------------------------------------------------\n",
            "2021/08/11\t19:36:51\n",
            "-----------------------------------------------------\n",
            "Processing file: ParallelLoopChunksOf1.java\n",
            "-----------------------------------------------------\n",
            "Processing 1st Phase: Parse and Normalisation\n",
            "Processing 2nd Phase: Symbol scoping visiting\n",
            "Processing 3rd Phase: Pyjama code translation visiting\n",
            "Processing 4th Phase: Generating java code\n",
            "Paralleled .class file is generated.\n",
            "Processing Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "146eEyqOt7HR",
        "outputId": "3c919fec-3432-4406-eff4-f60aa248a531"
      },
      "source": [
        "!java -cp Pyjama/Pyjama.jar:. ParallelLoopChunksOf1 4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Thread 3 performed iteration 3\n",
            "Thread 1 performed iteration 1\n",
            "Thread 1 performed iteration 5\n",
            "Thread 1 performed iteration 9\n",
            "Thread 1 performed iteration 13\n",
            "Thread 3 performed iteration 7\n",
            "Thread 3 performed iteration 11\n",
            "Thread 3 performed iteration 15\n",
            "Thread 2 performed iteration 2\n",
            "Thread 2 performed iteration 6\n",
            "Thread 2 performed iteration 10\n",
            "Thread 2 performed iteration 14\n",
            "Thread 0 performed iteration 0\n",
            "Thread 0 performed iteration 4\n",
            "Thread 0 performed iteration 8\n",
            "Thread 0 performed iteration 12\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Kt0EBxruM4o"
      },
      "source": [
        "## Another way to do round-robin scheduling\n",
        "\n",
        "At this point, you might have guessed that `#omp parallel` creates the threads and `#omp for` divides the work according to the schedule specified (the default is `static` with chunk size equals to the number of iterations divided by the number of threads). With this in mind, we can write code that will do round-robin scheduling of the for loop without using `#omp for`. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_i2SGSav46M",
        "outputId": "f5095022-761d-42dd-f3f8-a89aa2e474e8"
      },
      "source": [
        "%%writefile ParallelLoopChunksOf1.java\n",
        "class ParallelLoopChunksOf1 {\n",
        "    final static int REPS = 16;\n",
        "    public static void main(String[] args) {\n",
        "        if (args.length >= 1) {\n",
        "            Pyjama.omp_set_num_threads(Integer.parseInt(args[0]));\n",
        "        }\n",
        "        System.out.println();\n",
        "\n",
        "        //#omp parallel\n",
        "        {\n",
        "            int numThreads = Pyjama.omp_get_num_threads();\n",
        "            int id = Pyjama.omp_get_thread_num();\n",
        "            for (int i = id; i < REPS; i+=numThreads) {\n",
        "                System.out.println(\"Thread \"+id+\" performed iteration \"+i);\n",
        "            }\n",
        "        }\n",
        "        System.out.println();\n",
        "    }\n",
        "}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting ParallelLoopChunksOf1.java\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6oRwa8EwDob",
        "outputId": "d3d1a993-3d86-45c9-892a-738c87388205"
      },
      "source": [
        "!java -jar Pyjama/Pyjama.jar ParallelLoopChunksOf1.java"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pyjama Compiler Version: 3.1.0\n",
            "-----------------------------------------------------\n",
            "2021/08/11\t19:46:28\n",
            "-----------------------------------------------------\n",
            "Processing file: ParallelLoopChunksOf1.java\n",
            "-----------------------------------------------------\n",
            "Processing 1st Phase: Parse and Normalisation\n",
            "Processing 2nd Phase: Symbol scoping visiting\n",
            "Processing 3rd Phase: Pyjama code translation visiting\n",
            "Processing 4th Phase: Generating java code\n",
            "Paralleled .class file is generated.\n",
            "Processing Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFANxSybwIiY",
        "outputId": "9e8a0f58-a795-47bc-8e1e-3a153e06e94b"
      },
      "source": [
        "!java -cp Pyjama/Pyjama.jar:. ParallelLoopChunksOf1 4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Thread 3 performed iteration 3\n",
            "Thread 0 performed iteration 0\n",
            "Thread 0 performed iteration 4\n",
            "Thread 0 performed iteration 8\n",
            "Thread 0 performed iteration 12\n",
            "Thread 2 performed iteration 2\n",
            "Thread 1 performed iteration 1\n",
            "Thread 3 performed iteration 7\n",
            "Thread 2 performed iteration 6\n",
            "Thread 1 performed iteration 5\n",
            "Thread 1 performed iteration 9\n",
            "Thread 1 performed iteration 13\n",
            "Thread 2 performed iteration 10\n",
            "Thread 2 performed iteration 14\n",
            "Thread 3 performed iteration 11\n",
            "Thread 3 performed iteration 15\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPUwcDDkwAzy"
      },
      "source": [
        "Compare the work assignment in this program with the previous program that uses `#omp for` directive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4V9l3PXwwYv0"
      },
      "source": [
        "## Dynamic Scheduling\n",
        "\n",
        "In some cases, it is beneficial for the assignment of loop iterations to occur at run-time. This is especially useful when each iteration of the loop can take a different amount of time. Dynamic scheduling, or assigning iterations to threads at run time, allows threads that have finished work to start on new work, while letting threads that are still busy continue to work in peace.\n",
        "\n",
        "An example of program that will benefit from this scheduling is shown below. This program counts the number of prime numbers between 1 and n. The amount of work involved in checking if a number is a prime number depends on the value of the number -- larger number will require more work. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6yVgZcWw3T-",
        "outputId": "386dd73d-e2e0-4ad7-fa18-6d18368174f1"
      },
      "source": [
        "%%writefile SimpleDynamicScheduling.java\n",
        "class SimpleDynamicScheduling {\n",
        "\n",
        "    static void sleepALittle(int numMillis) {\n",
        "        try { \n",
        "            Thread.sleep(numMillis); \n",
        "        } catch(InterruptedException e) {\n",
        "            // do nothing\n",
        "        }\n",
        "    }\n",
        "\n",
        "    public static void main(String[] args) {\n",
        "        int numThreads = Pyjama.omp_get_num_procs();\n",
        "        if (args.length >= 1) {\n",
        "            numThreads = Integer.parseInt(args[0]);\n",
        "        }\n",
        "\n",
        "        long startTime = System.currentTimeMillis();\n",
        "        int count = 1;\n",
        "\n",
        "        //#omp parallel for num_threads(numThreads) /* schedule(dynamic)  */\n",
        "        for(int i = 1; i <= 100; i++) {\n",
        "            sleepALittle(i);\n",
        "        }\n",
        "    \n",
        "        long endTime = System.currentTimeMillis();        \n",
        "        System.out.println(\"Time = \" + (endTime-startTime) + \" ms\");\n",
        "\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting SimpleDynamicScheduling.java\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqh4CVQow3sJ"
      },
      "source": [
        "To employ a dynamic scheduling policy, you can specify `schedule(dynamic)` or `schedule(dynamic, chunkSize)` instead of `schedule(static)` or `schedule(static, chunkSize)`. Try specifying dynamic scheduling and different chunk size and see what happens to the program's runtime.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITg5R2VjxkjE",
        "outputId": "e09ad210-8471-4477-c380-a4e4d29ae7f3"
      },
      "source": [
        "!java -jar Pyjama/Pyjama.jar SimpleDynamicScheduling.java"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pyjama Compiler Version: 3.1.0\n",
            "-----------------------------------------------------\n",
            "2021/08/12\t03:22:19\n",
            "-----------------------------------------------------\n",
            "Processing file: SimpleDynamicScheduling.java\n",
            "-----------------------------------------------------\n",
            "Processing 1st Phase: Parse and Normalisation\n",
            "Processing 2nd Phase: Symbol scoping visiting\n",
            "Processing 3rd Phase: Pyjama code translation visiting\n",
            "Processing 4th Phase: Generating java code\n",
            "Paralleled .class file is generated.\n",
            "Processing Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUb8iLaGxnn8",
        "outputId": "3ce2ae67-3cfb-49a3-f94f-04db37d2e174"
      },
      "source": [
        "!java -cp Pyjama/Pyjama.jar:. SimpleDynamicScheduling 4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time = 2219 ms\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}