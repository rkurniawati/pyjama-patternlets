{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rkurniawati/pyjama-patternlets/blob/master/Java_OpenMP_Patternlets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIYdn1woOS1n"
      },
      "source": [
        "# Java OpenMP Patternlet Notebook\n",
        "\n",
        "Adapted to Java by Ruth Kurniawati (Westfield State University) based on the [PDC book](https://pdcbook.calvin.edu/pdcbook/RaspberryPiHandout/) from [CSInParallel](https://csinparallel.org/index.html). \n",
        "\n",
        "This notebook contains OpenMP patternlet examples in the Java programming language. Patterns are reusable solution for commonly occuring problems. The OpenMP patternlets are reusable solution that were originally written in the C language with the OpenMP library by Joel Adams: \n",
        "\n",
        "> Adams, Joel C. \"Patternlets: A Teaching Tool for Introducing Students to Parallel Design Patterns.\" 2015 IEEE International Parallel and Distributed Processing Symposium Workshop. IEEE, 2015.\n",
        "\n",
        "However, OpenMP library is only available for C/C++ and Fortran languages. For Java, Pyjama provides support for OpenMP-like directive. More information about Pyjama can be found in the paper below:\n",
        "\n",
        "> Vikas, Nasser Giacaman, and Oliver Sinnen. 2013. Pyjama: OpenMP-like implementation for Java, with GUI extensions. In <i>Proceedings of the 2013 International Workshop on Programming Models and Applications for Multicores and Manycores</i> (<i>PMAM '13</i>). Association for Computing Machinery, New York, NY, USA, 43–52. DOI:https://doi.org/10.1145/2442992.2442997"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MmtgBkPiZrw"
      },
      "source": [
        "# Multicore Systems and Multi-Threading\n",
        "\n",
        "Before proceeding with the examples, let's investigate the computer that this notebook is running on. For this, let's use the `lscpu` command. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khorTJcRlozK"
      },
      "outputs": [],
      "source": [
        "!lscpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY1glALCX8qK"
      },
      "source": [
        "## Cores, Processes and Threads\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo98NtHglr9y"
      },
      "source": [
        "If you run `lscpu` in the notebook, you may see output similar to below:\n",
        "\n",
        "```\n",
        "Architecture:        x86_64\n",
        "CPU op-mode(s):      32-bit, 64-bit\n",
        "Byte Order:          Little Endian\n",
        "CPU(s):              2\n",
        "On-line CPU(s) list: 0,1\n",
        "Thread(s) per core:  2\n",
        "Core(s) per socket:  1\n",
        "Socket(s):           1\n",
        "NUMA node(s):        1\n",
        "Vendor ID:           GenuineIntel\n",
        "CPU family:          6\n",
        "Model:               79\n",
        "Model name:          Intel(R) Xeon(R) CPU @ 2.20GHz\n",
        "Stepping:            0\n",
        "CPU MHz:             2199.998\n",
        "BogoMIPS:            4399.99\n",
        "Hypervisor vendor:   KVM\n",
        "Virtualization type: full\n",
        "L1d cache:           32K\n",
        "L1i cache:           32K\n",
        "L2 cache:            256K\n",
        "L3 cache:            56320K\n",
        "NUMA node0 CPU(s):   0,1\n",
        "Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
        "```\n",
        "\n",
        "This output means that the computer has 2 CPUs -- however, there is actually only one physical core but this core can execute 2 execution threads. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdhYtKRaYTBj"
      },
      "source": [
        "The `lscpu` command tells us a LOT of useful information, including the number of available cores. In this case we know that there is one socket (or chip) with 1 physical core, where each core can support 2 thread. On larger systems, it is common to see multiple threads supported per core. This is an example of **simultaneous multi-threading** (SMT, or Hyperthreading on Intel systems). A **core** can be thought of as the compute unit of the CPU. It includes registers, an ALU, and control units.\n",
        "\n",
        "Before we can discuss what a thread is, we must first discuss what a process is. A **process** can be thought of as an abstraction of a running program. When you type a command into the command line and press Enter, the Bash shell launches a process associated with that program executable. Each process contains a copy of the code and data of the program executable, and its own allocation of the stack and heap.\n",
        "\n",
        "A **thread** is a light-weight process. While each thread gets its own stack allocation, it shares the heap, code and data of the parent process. As a result, all the threads in a multi-threaded process can access a common pool of memory. This is why multi-threading is commonly referred to as shared memory programming. A single-threaded process is also referred to as a serial process or program."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4BT6lHhc3Dv"
      },
      "source": [
        "### Process Execution\n",
        "A multicore CPU allows multiple processes to execute simultaneously, or in **parallel**. While the terms concurrency and parallel are related, it is useful to think of concurrency as a software/OS-level concept, while parallel as a hardware/execution concept. A multi-threaded program, while capable of parallel execution, runs concurrently on a system with only a single CPU core."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaLDQy2CdIB1"
      },
      "source": [
        "## Thread Execution\n",
        "\n",
        "The primary goal of creating multi-threaded programs is to decrease the speed of a program’s execution. In a program that is perfectly parallelizable (that is, all components are paralleizable), it is usually possible to distribute the work associated with a program equally among all the threads. For a program _p_ whose work is equally distributed among _t_ threads, it will take roughly _p_/_t_ time, if executed on _t_ cores.\n",
        "\n",
        "For example, if a multi-threaded process that is perfectly parallelized takes 100 seconds to execute on one core, on a multi-core system with 4 cores the program will take approximately 100/4 seconds = 25 seconds to execute."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iskUMiEdtbk"
      },
      "source": [
        "## Leveraging Multiple Cores\n",
        "While multicore processors are ubiquitous in today’s world, most of the popular programming languages were designed to support single-thread execution. However, several native libraries are available for supporting multi-threading in popular languages like C/C++ and FORTRAN.\n",
        "\n",
        "One of these libraries is the Open MultiProcessing (OpenMP), a popular API for shared memory programming, and a standard since 1997. A key benefit of OpenMP over explicit threading libraries like POSIX threads is the ability to incrementally add parallelism to a program. For standard threaded programs, it is usually necessary to write a lot of extra code to add multi-threading to a program. Instead, OpenMP employs a series of pragmas, or special compiler directives, that tell the compiler how to parallelize the code.\n",
        "\n",
        "OpenMP library is only available for C/C++ and Fortran languages. For Java, Pyjama compiler and runtime provide support for OpenMP-like directive. More information about Pyjama can be found in the paper below:\n",
        "\n",
        "Vikas, Nasser Giacaman, and Oliver Sinnen. 2013. Pyjama: OpenMP-like implementation for Java, with GUI extensions. In <i>Proceedings of the 2013 International Workshop on Programming Models and Applications for Multicores and Manycores</i> (<i>PMAM '13</i>). Association for Computing Machinery, New York, NY, USA, 43–52. DOI:https://doi.org/10.1145/2442992.2442997\n",
        "\n",
        "In the rest of this notebook, we will look at Java programs that use OpenMP-like directives provided by the Pyjama to explore small patterns (_patternlets_) in parallel programming. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cF_wyE-XOR6"
      },
      "source": [
        "# Pyjama OpenMP library\n",
        "\n",
        "Before proceeding to the code sample, we need to make Pyjama compiler and runtime available to this notebook. \n",
        "\n",
        "The Pyjama library used in this notebook is obtained from the [CDER project](https://tcpp.cs.gsu.edu/curriculum/?q=node/21183), specifically [the version with additional bug fixes provided by Tennnessee Tech](https://www.csc.tntech.edu/pdcincs/index.php/installation). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rn4GBwkFqmIS"
      },
      "source": [
        "## Pyjama setup\n",
        "\n",
        "First, let's download and setup Pyjama Java source code compiler and runtime library. The commands below will download a ZIP file from Tennessee Tech, unzip it, and create the Pyjama/Pyjama.jar link to point to the specific jar file extracted from the ZIP file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHHuztRRq05q"
      },
      "outputs": [],
      "source": [
        "!wget -O Pyjama.zip https://www.csc.tntech.edu/pdcincs/resources/modules/tools/updated/Pyjama.zip\n",
        "!unzip -o -d lib Pyjama.zip \n",
        "!ln lib/Pyjama/Pyjama-3.1.0.jar lib/Pyjama.jar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6wdCiatXgDP"
      },
      "source": [
        "## Hello world\n",
        "\n",
        "In this example, we will verify that the Pyjama installation is working and able to create multiple threads as specified in the `#omp parallel num_threads` directive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNiwv0NofcOe"
      },
      "outputs": [],
      "source": [
        "%%writefile HelloWorld.java\n",
        "public class HelloWorld\n",
        "{\n",
        "\t\n",
        "\tpublic static void main(String[] args) \n",
        "\t{\n",
        " \n",
        "    Pyjama.omp_set_num_threads(10);\n",
        "\t\t//#omp parallel\n",
        "\t\t{\n",
        "\t\t\tint id = Pyjama.omp_get_thread_num();\n",
        "      int numThreads = Pyjama.omp_get_num_threads();\n",
        "      System.out.print(\"Hello from thread \" + id+ \", \");\n",
        "      System.out.println(\"of a total of \"+ numThreads+ \" threads.\");\n",
        "\t\t}\n",
        "\t}\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGagshBWfvIV"
      },
      "source": [
        "Note that the OpenMP directive is specified inside a single line comment that starts with `//`. For the directive to be recognized by Pyjama compiler, the `//` has to be followed immediately by `#omp`. \n",
        "\n",
        "First, let's use the Pyjama compiler to process the `#omp parallel` directive in the program."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNQrDeHAgOT4"
      },
      "outputs": [],
      "source": [
        "!java -jar lib/Pyjama.jar HelloWorld.java"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InLU3iqVgd2t"
      },
      "source": [
        "Now, we're ready to run the HelloWorld program. To do this, you will need to make the Pyjama.jar available in the classpath so that the Pyjama OpenMP-like runtime library is available to the HelloWorld program."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQHv7NowgkCV"
      },
      "outputs": [],
      "source": [
        "!java -cp lib/Pyjama.jar:. HelloWorld"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnQHI_TWgrWL"
      },
      "source": [
        "You should expect 10 lines of `Hello from thread x, of a total of y threads`. The lines may be interspersed with one another if you have more than one processors running the code. An example output is below.\n",
        "\n",
        "```\n",
        "Hello from thread 9, Hello from thread 5, Hello from thread 8, Hello from thread 2, of a total of 10 threads.\n",
        "of a total of 10 threads.\n",
        "Hello from thread 6, Hello from thread 3, of a total of 10 threads.\n",
        "Hello from thread 4, of a total of 10 threads.\n",
        "of a total of 10 threads.\n",
        "of a total of 10 threads.\n",
        "Hello from thread 7, of a total of 10 threads.\n",
        "Hello from thread 0, of a total of 10 threads.\n",
        "Hello from thread 1, of a total of 10 threads.\n",
        "of a total of 10 threads.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHnm-q8fp7ta"
      },
      "source": [
        "# A Simple Parallel Program\n",
        "\n",
        "## The SPMD Patternlet\n",
        "\n",
        "\n",
        "A patternlet is a small program that succinctly illustrates common patterns in parallel programming. The first patternlet we will study is Single Program, Multiple Data (SPMD). Let’s start by examining Spmd2.java, a program that uses OpenMP pragmas to make it easy to run a portion of the program on multiple threads. Note that the variables `id` and `numThreads` are shared among the threads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7xCZbDnqVAC"
      },
      "outputs": [],
      "source": [
        "%%writefile Spmd2.java\n",
        "class Spmd2 {\n",
        "    public static void main(String[] args) {\n",
        "        if (args.length >= 1) {\n",
        "            Pyjama.omp_set_num_threads(Integer.parseInt(args[0]));\n",
        "        }\n",
        "        System.out.println();\n",
        "\n",
        "        int id, numThreads;\n",
        "        //#omp parallel shared(id, numThreads)\n",
        "        {\n",
        "            numThreads = Pyjama.omp_get_num_threads();\n",
        "            id = Pyjama.omp_get_thread_num();\n",
        "            System.out.println(\"Hello from thread \"+ id +\" of \" + numThreads);\n",
        "        }\n",
        "\n",
        "        System.out.println();\n",
        "    }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blePwuMok2OP"
      },
      "source": [
        "The `omp parallel` directive tells the Pyjama compiler that the block of code within the curly braces be run on separate threads. Prior to the line with the directive, the program is run serially. When block of code marked with the `omp parallel` directive executes, Pyjama generates a a team of threads (known as forking). Each thread is assigned its own id and runs separate copies of the code between the curly braces. At the end of the block scope, Pyjama combines all the threads together to a single-threaded process (known as joining). Conceptually, the process looks like the following."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1Tpt04ZrOQ6"
      },
      "source": [
        "<img src=https://pdcbook.calvin.edu/pdcbook/RaspberryPiHandout/_images/ForkJoin_SPMD.png >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJr5ChxylkDw"
      },
      "source": [
        "## Running the Program"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMm8AUZssydy"
      },
      "source": [
        "Just like in the HelloWorld example, first we need to use the Pyjama compiler to process the `#omp` directive. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBferIsmrIRC"
      },
      "outputs": [],
      "source": [
        "!java -jar lib/Pyjama.jar Spmd2.java"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc0C9Q-Lsrg6"
      },
      "source": [
        "Now, we're ready to run the Spmd2 program. Let's specify that you'd like to have 10 threads by supplying this number in the command line argument. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOLa2FNvrNjK"
      },
      "outputs": [],
      "source": [
        "!java -cp lib/Pyjama.jar:. Spmd2 10 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ORBtUPclotz"
      },
      "source": [
        "Try running the program a few times with 10 threads (press the run button in the cell above). Observe the output. Occasionally something will be amiss. Do you notice it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1hj5AT8l4jg"
      },
      "source": [
        "## Race Conditions\n",
        "\n",
        "Watch this [video](\n",
        "https://d32ogoqmya1dw8.cloudfront.net/files/csinparallel/raceconditions_workshop.mov) to help you understand what's going on. Note that the video is made for the C++ version of the program, however the underlying issue is the same. \n",
        "\n",
        "The Spmd2 program has a race condition where there are more than one threads modifying a shared variable. Which shared variable(s) is/are causing the problem?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KexJJTwunv0T"
      },
      "source": [
        "## Fixing the code\n",
        "\n",
        "For this example, the race condition can be avoided by ensuring that each threads has its own copy of `id` and `numThreads` variables. Instead of declaring them as `shared` in the `omp parallel` directive, use the `private` clause as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFBEy0tppsYa"
      },
      "outputs": [],
      "source": [
        "%%writefile Spmd2.java\n",
        "class Spmd2 {\n",
        "    public static void main(String[] args) {\n",
        "        if (args.length >= 1) {\n",
        "            Pyjama.omp_set_num_threads(Integer.parseInt(args[0]));\n",
        "        }\n",
        "        System.out.println();\n",
        "\n",
        "        int id, numThreads;\n",
        "        //#omp parallel private(id, numThreads)\n",
        "        {\n",
        "            numThreads = Pyjama.omp_get_num_threads();\n",
        "            id = Pyjama.omp_get_thread_num();\n",
        "            System.out.println(\"Hello from thread \"+ id +\" of \" + numThreads);\n",
        "        }\n",
        "\n",
        "        System.out.println();\n",
        "    }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWHF-hfyp3S9"
      },
      "source": [
        "Let's compile and run this modified program."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-pLKA_Hp6GP"
      },
      "outputs": [],
      "source": [
        "!java -jar lib/Pyjama.jar Spmd2.java"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnwWKH6hp87h"
      },
      "outputs": [],
      "source": [
        "!java -cp lib/Pyjama.jar:. Spmd2 10 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6-zqcZlqACB"
      },
      "source": [
        "Were you able to reproduce the race condition using the corrected program? Why should you also declare `numThreads` as a private variable?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-qDxBrjqVqA"
      },
      "source": [
        "# Running Loops in Parallel\n",
        "\n",
        "Next we will consider a program that has a loop in it. An iterative for loop is a remarkably common pattern in all programming, primarily used to perform a calculation N times, often over a set of data containing N elements, using each element in turn inside the for loop.\n",
        "\n",
        "If there are no dependencies between the iterations (i.e. the order of them is not important), then the code inside the loop can be split between forked threads. However, the programmer must first decide how to partition the work between the threads. Specifically, how many and which iterations of the loop will each thread complete on its own?\n",
        "\n",
        "The **data decomposition** pattern describes the way how work is distributed across multiple threads. This chapter presents two patternlets, parallelLoop-equalChunks and parallelLoop-chunksOf1, that describe two common data decomposition strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bpf2GVFqrFk"
      },
      "source": [
        "## Parallel Loop, Equal Chunks\n",
        "\n",
        "Let's experiment with another OpenMP directive that will divide the work in a loop into equal chunks. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UQ7T_Nlq5ZS"
      },
      "outputs": [],
      "source": [
        "%%writefile ParallelLoopEqualChunks.java\n",
        "class ParallelLoopEqualChunks {\n",
        "    final static int REPS = 16;\n",
        "    public static void main(String[] args) {\n",
        "        if (args.length >= 1) {\n",
        "            Pyjama.omp_set_num_threads(Integer.parseInt(args[0]));\n",
        "        }\n",
        "        System.out.println();\n",
        "\n",
        "        //#omp parallel for  \n",
        "        for (int i = 0; i < REPS; i++) {\n",
        "            int id = Pyjama.omp_get_thread_num();\n",
        "            System.out.println(\"Thread \"+id+\" performed iteration \"+i);\n",
        "        }\n",
        "\n",
        "        System.out.println();\n",
        "    }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADAdiP5UrDXi"
      },
      "source": [
        "The `omp parallel for` directive tells the Pyjama OpenMP compiler to do the following:\n",
        "- Generate a team of threads (default is equal to the number of cores)\n",
        "- Assign each thread an equal number of iterations (a chunk) of the for loop.\n",
        "- At the end of the scope of the for loop, join all the theads back to a single-threaded process.\n",
        "\n",
        "As in our previous example, the code up to the `omp parallel for` directive is run serially. The code that is in the scope of the `omp parallel for` directive (everything inside the for loop) is run in parallel, with a subset of iterations assigned to each thread. After the implicit join at the end of the for loop, the program once again is a single-threaded process that executes serially to completion.\n",
        "\n",
        "In the above program, REPS is set to 16. If the program is run with 4 threads, then each thread gets 4 iterations of the loop (see illustration below):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz0dEXBPrhCg"
      },
      "source": [
        "<img src=\"https://pdcbook.calvin.edu/pdcbook/RaspberryPiHandout/_images/ParallelFor_Chunks-4_threads-1.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUAmJL3KrmPh"
      },
      "source": [
        "## Try It Out\n",
        "\n",
        "Try compile and run the program using the following commands below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drSrg4qirvUz"
      },
      "outputs": [],
      "source": [
        "!java -jar lib/Pyjama.jar ParallelLoopEqualChunks.java"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TuG5KOUr07X"
      },
      "outputs": [],
      "source": [
        "!java -cp lib/Pyjama.jar:. ParallelLoopEqualChunks 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZifCPWfEr-Vl"
      },
      "source": [
        "Try running the program a few times with 4 threads. How does the work in the for loop get assigned to the threads?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rrR9m6esSun"
      },
      "source": [
        "### Unequal Iterrations\n",
        "\n",
        "Also try using a different number of threads. Pick a number so that the number iterations cannot be equally divided by the number of threads, such as 5. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmcjcOXEsl8H"
      },
      "outputs": [],
      "source": [
        "!java -cp lib/Pyjama.jar:. ParallelLoopEqualChunks 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWgqjv6HsuTZ"
      },
      "source": [
        "What happens to the extra iterations?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmO_5IZxs1R5"
      },
      "source": [
        "This equal-chunk decomposition is especially useful in the following scenarios:\n",
        "- Each iteration of the loop takes the same amount of time to finish\n",
        "- The loop involves accesses to data in consecutive memory locations (e.g. an array), allowing the program to take advantage of spatial locality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfmlAD0ntBTy"
      },
      "source": [
        "## Parallel Loop, Chunks of 1\n",
        "\n",
        "In some cases, it makes sense to have iterations assigned to threads in “round-robin” style. In other words, iteration 0 goes to thread 0, iteration 1 goes to thread 1, iteration 2 goes to thread 2, and so on.\n",
        "\n",
        "Let's examine the code below that directs Pyjama to do this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92YGOmQOtStE"
      },
      "outputs": [],
      "source": [
        "%%writefile ParallelLoopChunksOf1.java\n",
        "class ParallelLoopChunksOf1 {\n",
        "    final static int REPS = 16;\n",
        "    public static void main(String[] args) {\n",
        "        if (args.length >= 1) {\n",
        "            Pyjama.omp_set_num_threads(Integer.parseInt(args[0]));\n",
        "        }\n",
        "        System.out.println();\n",
        "\n",
        "        //#omp parallel for schedule(static,1)\n",
        "        for (int i = 0; i < REPS; i++) {\n",
        "            int id = Pyjama.omp_get_thread_num();\n",
        "            System.out.println(\"Thread \"+id+\" performed iteration \"+i);\n",
        "        }\n",
        "\n",
        "        System.out.println();\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akcD6-Qqtfcn"
      },
      "source": [
        "The code is nearly identical to the previous program. The difference is in the `omp` directive. The `omp parallel for` directive has a new `schedule` clause which specifies the way iterations should be assigned to threads. The `static` keyword indicates that the the compiler should assign work to each thread at compile time (a **static** scheduling policy). The `1` indicates that the chunk size should be 1 iteration. Therefore, the above code would have 16 total chunks.\n",
        "\n",
        "In the case where the number of chunks exceed the number of theads, each successive chunk is assigned to a thread in round-robin fashion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tORVRFXutPEv"
      },
      "source": [
        "### Try It Out\n",
        "\n",
        "Let's compile and run the code. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmBOb2wQt2BI"
      },
      "outputs": [],
      "source": [
        "!java -jar lib/Pyjama.jar ParallelLoopChunksOf1.java"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "146eEyqOt7HR"
      },
      "outputs": [],
      "source": [
        "!java -cp lib/Pyjama.jar:. ParallelLoopChunksOf1 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Kt0EBxruM4o"
      },
      "source": [
        "## Another way to do round-robin scheduling\n",
        "\n",
        "At this point, you might have guessed that `#omp parallel` creates the threads and `#omp for` divides the work according to the schedule specified (the default is `static` with chunk size equals to the number of iterations divided by the number of threads). With this in mind, we can write code that will do round-robin scheduling of the for loop without using `#omp for`. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_i2SGSav46M"
      },
      "outputs": [],
      "source": [
        "%%writefile ParallelLoopChunksOf1.java\n",
        "class ParallelLoopChunksOf1 {\n",
        "    final static int REPS = 16;\n",
        "    public static void main(String[] args) {\n",
        "        if (args.length >= 1) {\n",
        "            Pyjama.omp_set_num_threads(Integer.parseInt(args[0]));\n",
        "        }\n",
        "        System.out.println();\n",
        "\n",
        "        //#omp parallel\n",
        "        {\n",
        "            int numThreads = Pyjama.omp_get_num_threads();\n",
        "            int id = Pyjama.omp_get_thread_num();\n",
        "            for (int i = id; i < REPS; i+=numThreads) {\n",
        "                System.out.println(\"Thread \"+id+\" performed iteration \"+i);\n",
        "            }\n",
        "        }\n",
        "        System.out.println();\n",
        "    }\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6oRwa8EwDob"
      },
      "outputs": [],
      "source": [
        "!java -jar lib/Pyjama.jar ParallelLoopChunksOf1.java"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFANxSybwIiY"
      },
      "outputs": [],
      "source": [
        "!java -cp lib/Pyjama.jar:. ParallelLoopChunksOf1 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPUwcDDkwAzy"
      },
      "source": [
        "Compare the work assignment in this program with the previous program that uses `#omp for` directive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4V9l3PXwwYv0"
      },
      "source": [
        "## Dynamic Scheduling\n",
        "\n",
        "In some cases, it is beneficial for the assignment of loop iterations to occur at run-time. This is especially useful when each iteration of the loop can take a different amount of time. Dynamic scheduling, or assigning iterations to threads at run time, allows threads that have finished work to start on new work, while letting threads that are still busy continue to work in peace.\n",
        "\n",
        "An example of program that will benefit from this scheduling is shown below. This program counts the number of prime numbers between 1 and n. The amount of work involved in checking if a number is a prime number depends on the value of the number -- larger number will require more work. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6yVgZcWw3T-"
      },
      "outputs": [],
      "source": [
        "%%writefile SimpleDynamicScheduling.java\n",
        "class SimpleDynamicScheduling {\n",
        "\n",
        "    static void sleepALittle(int numMillis) {\n",
        "        try { \n",
        "            Thread.sleep(numMillis); \n",
        "        } catch(InterruptedException e) {\n",
        "            // do nothing\n",
        "        }\n",
        "    }\n",
        "\n",
        "    public static void main(String[] args) {\n",
        "        int numThreads = Pyjama.omp_get_num_procs();\n",
        "        if (args.length >= 1) {\n",
        "            numThreads = Integer.parseInt(args[0]);\n",
        "        }\n",
        "\n",
        "        long startTime = System.currentTimeMillis();\n",
        "        int count = 1;\n",
        "\n",
        "        //#omp parallel for num_threads(numThreads) /* schedule(dynamic)  */\n",
        "        for(int i = 1; i <= 100; i++) {\n",
        "            sleepALittle(i);\n",
        "        }\n",
        "    \n",
        "        long endTime = System.currentTimeMillis();        \n",
        "        System.out.println(\"Time = \" + (endTime-startTime) + \" ms\");\n",
        "\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITg5R2VjxkjE"
      },
      "outputs": [],
      "source": [
        "!java -jar lib/Pyjama.jar SimpleDynamicScheduling.java"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUb8iLaGxnn8"
      },
      "outputs": [],
      "source": [
        "!java -cp lib/Pyjama.jar:. SimpleDynamicScheduling 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqh4CVQow3sJ"
      },
      "source": [
        "To employ a dynamic scheduling policy, you can specify `schedule(dynamic)` or `schedule(dynamic, chunkSize)` instead of `schedule(static)` or `schedule(static, chunkSize)`. Try specifying dynamic scheduling and different chunk size and see what happens to the program's runtime.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugMwRIBFn0cJ"
      },
      "source": [
        "# Parallel Sum\n",
        "\n",
        "Very often, loops are used with an accumulator variable to compute a a single value from a set of values, such as the sum of integers in an array or list. Fortunately, OpenMP implements a special parallel pattern known as *reduction*, which will aid us in this process. The reduction pattern is one of a group of patterns called **collective communication** patterns because the threads must somehow work together to create the final desired single value.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhyoHP-VoGX6"
      },
      "source": [
        "## Initial Attempt\n",
        "\n",
        "Let's try implementing parallel sum without using the OpenMP's _reduction_ directive. \n",
        "\n",
        "The `Reduction0` program below will create an array with 1 million integers that are generated randomly in the [0-1000) range. Note that `sequentialSum` sums up the array sequentially. The code in `parallelSum` uses OpenMP's parallel and for directives to attempt to sum up the array in parallel. Examine the two functions carefully. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCHUZ8b3onCq"
      },
      "outputs": [],
      "source": [
        "%%writefile Reduction0.java\n",
        "import java.util.Random;\n",
        "\n",
        "class Reduction0 {\n",
        "    final static int SIZE=1000000;\n",
        "\n",
        "    public static void main(String[] args) {\n",
        "        if (args.length >= 1) {\n",
        "            Pyjama.omp_set_num_threads(Integer.parseInt(args[0]));\n",
        "        }\n",
        "        System.out.println();\n",
        "\n",
        "        // Generate SIZE random values in [0..1000) range\n",
        "        // Fix the seed to 123 so the set of numbers will be the same in each run.\n",
        "        int[] array = new Random(123).ints(SIZE, 0, 1000).toArray();\n",
        "        \n",
        "        System.out.println(\"Seq. sum: \\t\" + sequentialSum(array));\n",
        "        System.out.println(\"Par. sum: \\t\" + parallelSum(array));\n",
        "    } \n",
        "\n",
        "\n",
        "    /* sum the array sequentially */\n",
        "    static int sequentialSum(int[] a) {\n",
        "        int sum = 0;\n",
        "        for (int i = 0; i < a.length; i++) {\n",
        "            sum += a[i];\n",
        "        }\n",
        "        return sum;\n",
        "    }\n",
        "\n",
        "    /* sum the array using multiple threads */\n",
        "    static int parallelSum(int[] a) {\n",
        "        int sum = 0;\n",
        "        //#omp parallel shared(a,sum) \n",
        "        {\n",
        "            //#omp for \n",
        "            for(int i = 0; i < a.length;i++) {\n",
        "                sum += a[i];\n",
        "            }\n",
        "        }\n",
        "        return sum;\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHZYNCp1o8Bo"
      },
      "outputs": [],
      "source": [
        "!java -jar lib/Pyjama.jar Reduction0.java"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oo9BtilpPRs"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IixezkipIlr"
      },
      "outputs": [],
      "source": [
        "!java -cp .:lib/Pyjama.jar Reduction0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oHILB5LW7m6"
      },
      "source": [
        "Note that the sum from the sequentialSum is different from the sum from the parallelSum. Try running the program multiple times. \n",
        "\n",
        "Note that the set of random numbers generated are the same each time you run the program since we supplied the seed for the random number generator to be `123`.  \n",
        "\n",
        "Are the results of the `sequentialSum` function changing? How about the results of the `parallelSum` function? Can you explain why the results of sequential and parallel sum are different?\n",
        "\n",
        "How many threads were used to calculate the sum in `parallelSum`? What happens when you change this?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcYipzXeX951"
      },
      "source": [
        "## Fixing the parallelSum function\n",
        "\n",
        "Let's try a different version of the parallelSum function (below). Note that this version use the OpenMP `reduction` clause. \n",
        "\n",
        "The notion of a reduction comes from the mathematical operation _reduce_, in which a collection of values are combined into a single value via a common mathematical function. Summing up a collection of values is a natural example of reduction. \n",
        "\n",
        "In the modified program, the `reduction(+:sum)` clause tells Pyjama that a summation is being performed (the `+` operator) on the variable `sum`, which acts as the accumulator.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S72_dEZBZtC0"
      },
      "outputs": [],
      "source": [
        "%%writefile Reduction1.java\n",
        "import java.util.Random;\n",
        "\n",
        "class Reduction1 {\n",
        "    final static int SIZE=1000000;\n",
        "\n",
        "    public static void main(String[] args) {\n",
        "        if (args.length >= 1) {\n",
        "            Pyjama.omp_set_num_threads(Integer.parseInt(args[0]));\n",
        "        }\n",
        "        System.out.println();\n",
        "\n",
        "        // Generate SIZE random values in [0..1000) range\n",
        "        // Fix the seed to 123 so the set of numbers will be the same in each run.\n",
        "        int[] array = new Random(123).ints(SIZE, 0, 1000).toArray();\n",
        "\n",
        "        System.out.println(\"Seq. sum: \\t\" + sequentialSum(array));\n",
        "        System.out.println(\"Par. sum: \\t\" + parallelSum(array));\n",
        "    } \n",
        "\n",
        "\n",
        "    /* sum the array sequentially */\n",
        "    static int sequentialSum(int[] a) {\n",
        "        int sum = 0;\n",
        "        for (int i = 0; i < a.length; i++) {\n",
        "            sum += a[i];\n",
        "        }\n",
        "        return sum;\n",
        "    }\n",
        "\n",
        "    /* sum the array using multiple threads */\n",
        "    static int parallelSum(int[] a) {\n",
        "        int sum = 0;\n",
        "        //#omp parallel shared(a,sum) \n",
        "        {\n",
        "            //#omp for reduction(+:sum)\n",
        "            for(int i = 0; i < a.length;i++) {\n",
        "                sum += a[i];\n",
        "            }\n",
        "        }\n",
        "        return sum;\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChlUru_CZ6Zd"
      },
      "outputs": [],
      "source": [
        "!java -jar lib/Pyjama.jar Reduction1.java"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmc3AoQxcVOU"
      },
      "outputs": [],
      "source": [
        "!java -cp .:lib/Pyjama.jar Reduction1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qC_bn88cgPH"
      },
      "source": [
        "Try running the program a few times and compare the results of `sequentialSum` and `parallelSum` again. Are they the same? \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXhleGldc-J0"
      },
      "source": [
        "## Something to ponder\n",
        "\n",
        "What do you think happen when you specify the `reduction` clause? \n",
        "\n",
        "Can you fix `parallelSum` function so that it will produce the correct result without using the `reduction` clause?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4rSf3wbdax-"
      },
      "source": [
        "The sum example that we experimented with was a fairly small example. We will look at larger examples in the following sections. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uvQmDyvdl0V"
      },
      "source": [
        "# Integration using the Trapezoidal Rule\n",
        "\n",
        "## Estimating the Area Under a Curve\n",
        "\n",
        "As our next example, let’s look at the problem of estimating the area under a curve. If you have taken a calculus course, you may recognize this problem as the Riemann sum or Trapezoidal Rule, which approximates the area under the curve (i.e. the integral) by splitting the area under the curve into a series of trapezoids.\n",
        "\n",
        "In the following programs, we attempt to use the trapezoid rule to approximate the integral:\n",
        "\n",
        "  $\\int_0^{\\pi} sin(x)_{dx}$\n",
        "\n",
        "using $2^{20}$ equal subdivisions. The answer from this computation should be 2.0. This [video](https://d32ogoqmya1dw8.cloudfront.net/files/csinparallel/workshops/numerical_integration_1_thread.mov) shows how a single thread would solve this problem. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1VAUgOafVbo"
      },
      "source": [
        "In the following program, a single thread serially computes the area of each trapezoid and adds all the trapezoids together into one value. A Java implementation of this program may look like the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uqF6TD9fUgS"
      },
      "outputs": [],
      "source": [
        "%%writefile Integration.java\n",
        "class Integration {\n",
        "    static double f(double x) {\n",
        "        return Math.sin(x);\n",
        "    }\n",
        "    \n",
        "    public static void main(String[] args) {\n",
        "        //Variables\n",
        "\n",
        "        double a = 0.0, b = Math.PI;         //limits of integration\n",
        "        int n = 1048576;                //number of subdivisions = 2^20\n",
        "        double h = (b - a) / n;         //width of each subdivision\n",
        "        double integral;                // accumulates answer\n",
        "\n",
        "        integral = (f(a) + f(b))/2.0;  //initial value\n",
        "\n",
        "        //sum up all the trapezoids\n",
        "        for(int i = 1; i < n; i++) {\n",
        "            integral += f(a+i*h);\n",
        "        }\n",
        "\n",
        "        integral = integral * h;\n",
        "        System.out.printf(\"With %d trapezoids, our estimate of the integral from \\n\", n);\n",
        "        System.out.printf(\"%f to %f is %f\\n\", a,b,integral);\n",
        "    } \n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg_ZBNE2tvLI"
      },
      "source": [
        "Let's compile and run this as a normal Java program:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQGUeDZrtnRk"
      },
      "outputs": [],
      "source": [
        "!javac Integration.java\n",
        "!java Integration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Byn7aKFXuEBq"
      },
      "source": [
        "## Parallel Integration - First Attempt\n",
        "\n",
        "Let’s now consider how we can use multiple threads to approximate the area under the curve in parallel. One strategy would be to assign each thread a subset of the total set of subdivisions, so that each thread separately computes its assigned set of trapezoids.\n",
        "\n",
        "This [video](https://d32ogoqmya1dw8.cloudfront.net/files/csinparallel/workshops/numerical_integration_4_threads.mov) illustrates how 4 threads would work together to approximate the area under the curve.\n",
        "\n",
        "Here's an attempt to parallelize the serial version above using OpenMP and Pyjama:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwl56w-dwYvq"
      },
      "outputs": [],
      "source": [
        "%%writefile Integration1.java\n",
        "class Integration1 {\n",
        "    static double f(double x) {\n",
        "        return Math.sin(x);\n",
        "    }\n",
        "    \n",
        "    public static void main(String[] args) {\n",
        "        if (args.length >= 1) {\n",
        "            Pyjama.omp_set_num_threads(Integer.parseInt(args[0]));\n",
        "        }\n",
        "        System.out.println();\n",
        "\n",
        "        //Variables\n",
        "\n",
        "        double a = 0.0, b = Math.PI;         //limits of integration\n",
        "        int n = 1048576;                //number of subdivisions = 2^20\n",
        "        double h = (b - a) / n;         //width of each subdivision\n",
        "        double integral;                // accumulates answer\n",
        "\n",
        "        integral = (f(a) + f(b))/2.0;  //initial value\n",
        "\n",
        "        //sum up all the trapezoids\n",
        "\n",
        "        //#omp parallel for shared(a, h, n, integral)\n",
        "        for(int i = 1; i < n; i++) {\n",
        "            integral += f(a+i*h);\n",
        "        }\n",
        "\n",
        "        integral = integral * h;\n",
        "        System.out.printf(\"With %d trapezoids, our estimate of the integral from \\n\", n);\n",
        "        System.out.printf(\"%f to %f is %f\\n\", a,b,integral);\n",
        "    } \n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_QsY2hFwj07"
      },
      "outputs": [],
      "source": [
        "!java -jar lib/Pyjama.jar Integration1.java"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqqGo_jDwuxE"
      },
      "source": [
        "If our parallel program is implemented correctly, the program should estimate the area under the curve as 2.00, which would be identical to the output of the serial program.\n",
        "\n",
        "Try running the program with different number of threads: 4, 2, 1. When will it produce the correct/expected value (2.0)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnycCBXcwoSD"
      },
      "outputs": [],
      "source": [
        "!java -cp .:lib/Pyjama.jar Integration1 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ry-kHQ-w74-H"
      },
      "outputs": [],
      "source": [
        "!java -cp .:lib/Pyjama.jar Integration1 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSedoOBP752m"
      },
      "outputs": [],
      "source": [
        "!java -cp .:lib/Pyjama.jar Integration1 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLDMVd_Y8AH5"
      },
      "source": [
        "## Parallel Integration - Second Attempt\n",
        "\n",
        "Now, let's use the previously discussed `reduction` clause to fix the issue that we observed in previous section. Here's the improved version below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcmjxgXF8P50"
      },
      "outputs": [],
      "source": [
        "%%writefile Integration2.java\n",
        "class Integration2 {\n",
        "    static double f(double x) {\n",
        "        return Math.sin(x);\n",
        "    }\n",
        "    \n",
        "    public static void main(String[] args) {\n",
        "        if (args.length >= 1) {\n",
        "            Pyjama.omp_set_num_threads(Integer.parseInt(args[0]));\n",
        "        }\n",
        "        System.out.println();\n",
        "\n",
        "        //Variables\n",
        "\n",
        "        double a = 0.0, b = Math.PI;         //limits of integration\n",
        "        int n = 1048576;                //number of subdivisions = 2^20\n",
        "        double h = (b - a) / n;         //width of each subdivision\n",
        "        double integral;                // accumulates answer\n",
        "\n",
        "        integral = (f(a) + f(b))/2.0;  //initial value\n",
        "\n",
        "        //sum up all the trapezoids\n",
        "\n",
        "        //#omp parallel for shared(a, h, n) reduction(+:integral)\n",
        "        for(int i = 1; i < n; i++) {\n",
        "            integral += f(a+i*h);\n",
        "        }\n",
        "\n",
        "        integral = integral * h;\n",
        "        System.out.printf(\"With %d trapezoids, our estimate of the integral from \\n\", n);\n",
        "        System.out.printf(\"%f to %f is %f\\n\", a,b,integral);\n",
        "    } \n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOL2MR-C8Z1u"
      },
      "source": [
        "As with the previous reduction patternlet example, we again have an accumulator variable in the loop, this time called integral. Thus our reduction clause reads reduction(+:integral).\n",
        "\n",
        "Compile and run the above program with 4 threads using the following commands:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPKl9LmI8jVg"
      },
      "outputs": [],
      "source": [
        "!java -jar lib/Pyjama.jar Integration2.java"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b026TnL384Wa"
      },
      "outputs": [],
      "source": [
        "!java -cp .:lib/Pyjama.jar Integration2 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeqhURuY89Hu"
      },
      "source": [
        "Did you get the expected result? (2.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11dm9-Mu9AS9"
      },
      "source": [
        "**Note**: \n",
        "\n",
        "Because we did not explicitly add an additional static or dynamic clause to the pragma on line 24 on the working version above, the default behavior of assigning equal amounts of work doing consecutive iterations of the loop (i.e. static scheduling) was used to decompose the problem onto threads. In this case, since the number of trapezoids used was 1048576, then with 4 threads, thread 0 will do the first 1048576/4 trapezoids, thread 1 the next 1048576/4 trapezoids, and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3_pa6sD9VdX"
      },
      "source": [
        "# Performance Study - Drug Design Exemplar\n",
        "\n",
        "Let’s look at a larger example. An important problem in Biology is that of drug design. The goal is to find small molecules, called _ligands_, that are good candidates for use as drugs.\n",
        "\n",
        "This is a very rough simulation of a program to compute how well a set of short protein ligands (each a possible drug) matches a given longer protein string. In the real software programs that do this, the matching is quite sophisticated, targeting possible \"receptor\" sites on the protein.\n",
        "\n",
        "Here is an image illustrating the concept of the ligand (represented by small sticks in center) binding to areas of the protein (represented by ribbon structure):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oy4urTvV9cBt"
      },
      "source": [
        "<img src=\"https://pdcbook.calvin.edu/pdcbook/RaspberryPiHandout/_images/proteinligand.jpg\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yk1d0Ki9k0U"
      },
      "source": [
        "For the real versions of this code and our simulated case, the longer the ligand or the longer the protein, the longer it takes for the matching and score of the match to complete.\n",
        "\n",
        "We have created a default fake protein in the code. This can be changed on the command line.\n",
        "\n",
        "We create the list of possible ligands by choosing random lengths (controlled by the `maxligand` variable). Thus, each ligand can be of a different length."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h9xZ0tn94u4"
      },
      "source": [
        "## The Drug Design Exemplar\n",
        "\n",
        "Unlike previous code examples, our primary focus this section is analyzing the performance of the static and dynamic OpenMP programs.\n",
        "\n",
        "An implementation of the exemplar is below. Note that in addition to changing the number of threads, you should also try different scheduling strategy (static vs dynamic). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vob8rA-Oz41M"
      },
      "outputs": [],
      "source": [
        "%%writefile DDPyjama.java\n",
        "import java.util.Random;\n",
        "\n",
        "public class DDPyjama {\n",
        "    static Random rand = new Random(42);\n",
        "\n",
        "    static String[] cannedLigands = \n",
        "        {\"razvex\", \"qudgy\", \"afrs\", \"sst\", \"pgfht\", \"rt\", \n",
        "        \"id\", \"how\", \"aaddh\",  \"df\", \"os\", \"hid\", \n",
        "        \"sad\", \"fl\", \"rd\", \"edp\", \"dfgt\", \"spa\"};\n",
        "\n",
        "    // Ligand Score pair\n",
        "    static class LSPair {\n",
        "        String ligand;\n",
        "        int score;\n",
        "    \n",
        "        public LSPair(String ligand, int score) {\n",
        "            this.ligand = ligand;\n",
        "            this.score = score;\n",
        "        }\n",
        "\n",
        "        @Override\n",
        "        public String toString() {\n",
        "            return \"[\"+ligand+\",\"+score+\"]\";\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // returns arbitrary string of lower-case letters of length at most max_ligand\n",
        "    static String makeLigand(int maxLigandLength) {\n",
        "\n",
        "        int len = rand.nextInt(maxLigandLength+1);\n",
        "        \n",
        "        StringBuilder sb = new StringBuilder();\n",
        "        for (int i = 0;  i < len;  i++) \n",
        "            sb.append((char) ('a' + rand.nextInt(26)));  \n",
        "        return sb.toString();\n",
        "    }\n",
        "  \n",
        "    private static String[] generateLigands(int numLigands, int maxLigandLength, boolean useCanned) {\n",
        "        // If we use canned ligands, use as many of them as we can, then fill the rest with randomly generated ligands. \n",
        "        // Otherwise, create a set of ligands whose length randomly varies from 1 to args.maxLigand\n",
        "\n",
        "        String[] result = new String[numLigands];\n",
        "\n",
        "        if (useCanned) {\n",
        "            for(int i = 0; i < Math.min(numLigands, cannedLigands.length); i++) {\n",
        "                result[i] = cannedLigands[i];\n",
        "            }\n",
        "        }\n",
        "\n",
        "        for(int i = useCanned ? cannedLigands.length : 0; i < numLigands; i++) {\n",
        "            result[i] = makeLigand(maxLigandLength);\n",
        "        }\n",
        "        return result;\n",
        "    }\n",
        "\n",
        "    public static void main(String[] args) {\n",
        "\n",
        "        if (args.length != 4) {\n",
        "            System.out.println(\"Usage DDPyjama numThreads numLigands maxLigandLength protein useCanned printLigands\");\n",
        "\n",
        "            // the example string below is one of Dijkstra's famous quotes\n",
        "            System.out.println(\"   Example: java -cp .:Pyjama.jar DDPyjama 4 10 8 \\\"Simplicity is a great virtue but it requires hard work to achieve it and education to appreciate it\\\" false true\\n\");\n",
        "        }\n",
        "\n",
        "        int numThreads = 4;\n",
        "        if (args.length >= 1) {\n",
        "            numThreads = Integer.parseInt(args[0]);\n",
        "        }\n",
        "\n",
        "        int numLigands = 12;\n",
        "        if (args.length >= 2) {\n",
        "            numLigands = Integer.parseInt(args[1]);\n",
        "        }\n",
        "\n",
        "        int maxLigandLength = 6;\n",
        "        if (args.length >= 3) {\n",
        "            maxLigandLength = Integer.parseInt(args[2]);\n",
        "        }\n",
        "\n",
        "        String protein = \"the cat in the hat wore the hat to the cat hat party\";\n",
        "\n",
        "        if (args.length >= 4) {\n",
        "            protein = args[3];\n",
        "        }\n",
        "\n",
        "        System.out.println(\"Number of threads: \" + numThreads);\n",
        "        System.out.println(\"Number of ligands: \"+numLigands);\n",
        "        System.out.println(\"Max ligand length: \"+ maxLigandLength);\n",
        "        System.out.println(\"Protein: \"+ protein);\n",
        "        System.out.println();\n",
        "\n",
        "        // Things to do: \n",
        "        // 1. Generate the requested numLigands w/ maxLigandLength\n",
        "        // 2. Calculate the matching score for each ligand vs the given protein\n",
        "        //    Score is calculated based on the number of character in the ligand that\n",
        "        //    appears in the same order in the protein. \n",
        "        // 3. Find the ligand(s) with the highest score\n",
        "\n",
        "        long start = System.currentTimeMillis();\n",
        "        String[] ligands = generateLigands(numLigands, maxLigandLength, args.length >= 5 && args[4].equals(\"true\"));\n",
        "\n",
        "        // print the ligands if desired\n",
        "        if (args.length >= 6 && args[5].equals(\"true\")) {\n",
        "            System.out.println(\"Here are the ligands\");\n",
        "            for(String l : ligands) {\n",
        "                System.out.println(l);\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        // map each ligand to (ligand, score)\n",
        "        // also keep track of the maxScore \n",
        "        LSPair[] ligandsWScore = new LSPair[numLigands];\n",
        "        int maxScore = 0;\n",
        "\n",
        "        //#omp parallel for num_threads(numThreads) shared(numLigands, ligands, ligandsWScore, protein) reduction(max:maxScore) schedule(dynamic)\n",
        "        for(int i = 0; i < numLigands; i++) {\n",
        "            String ligand = ligands[i];\n",
        "            int score = calcScore(ligand, protein);\n",
        "            ligandsWScore[i] = new LSPair(ligand, score);\n",
        "            maxScore = Math.max(maxScore, score);\n",
        "        }\n",
        "\n",
        "        // find the ligands whose score is maxScore\n",
        "        // this is a reduce operation\n",
        "        StringBuilder sb = new StringBuilder();\n",
        "        for(int i = 0; i < numLigands; i++) {\n",
        "            if (ligandsWScore[i].score == maxScore) {\n",
        "                if (sb.length() > 0) sb.append(\", \");\n",
        "                sb.append(ligandsWScore[i].ligand);\n",
        "            }\n",
        "        }\n",
        "\n",
        "        long end = System.currentTimeMillis();\n",
        "        System.out.println(\"The maximum score is \" + maxScore);\n",
        "        System.out.println(\"Achieved by ligand(s) \"+ sb.toString());\n",
        "        System.out.println(\"Calculation time \" + (end-start) + \" ms\");\n",
        "    }\n",
        "\n",
        "    /**\n",
        "     * Match a ligand (str1) and the protein. Count the number of characters in str1\n",
        "     * that appear in the same seq in str2 (there can be any number of intervening chars)\n",
        "     * @param str1 first string\n",
        "     * @param str2 second string\n",
        "     * @return number of matches\n",
        "     */\n",
        "    private static int calcScore(String str1, String str2) {\n",
        "        // no match if either is empty string\n",
        "        if (str1.length() == 0 || str2.length() == 0) return 0;\n",
        "\n",
        "        if (str1.charAt(0) == str2.charAt(0)) {\n",
        "            return 1 + calcScore(str1.substring(1), str2.substring(1));\n",
        "        }\n",
        "        return Math.max(\n",
        "            calcScore(str1, str2.substring(1)), calcScore(str1.substring(1), str2));\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "heHlvVVdz-4h"
      },
      "outputs": [],
      "source": [
        "!java -jar lib/Pyjama.jar DDPyjama.java"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSDRO1s23pot"
      },
      "outputs": [],
      "source": [
        "!java -cp .:lib/Pyjama.jar DDPyjama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oic9JmgM34Hp"
      },
      "source": [
        "Try running it with more ligands and longer ligands length. Observe how much longer it takes for the program to finish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpCnmmRU4jgD"
      },
      "outputs": [],
      "source": [
        "!java -cp .:lib/Pyjama.jar DDPyjama 4 20 6 \"your time is limited, so don't waste it living someone else's life\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGSVDY776kxR"
      },
      "source": [
        "## Speedup\n",
        "\n",
        "**Speedup** is the ratio of the execution time of a serial program to its parallel execution. Specifically,\n",
        "\n",
        "  $S_n= \\frac{T_1}{T_n}$\n",
        "\n",
        "Where $T_1$ is the time it takes to execute a program on 1 thread, while $T_n$ is the time it takes to execute a program on n threads. Some important notes:\n",
        "\n",
        " - A speedup greater than 1 indicates that there is benefit to the parallel implementation. A speedup of x means that the parallel code is x times faster.\n",
        " - A speedup less than 1 indicates that there is no benefit to parallelism.\n",
        "\n",
        "To correctly calculate speedup, we must first measure the running time of a program. In this case, we will use `System.currentTimeMillis` to measure how long it takes for the program to finish, starting from the ligand generation step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZoo1ytW73Sk"
      },
      "source": [
        "## Performance study\n",
        "\n",
        " Try running the program using different number of threads and different scheduling strategy and complete the table below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsUPVw_U8o3p"
      },
      "source": [
        "Time (s) | 1 Thread | 2 Threads | 3 Threads | 4 Threads\n",
        "---------|----------|-----------|-----------|----------\n",
        "dynamic  |          |           |           |\n",
        "static   |          |           |           |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMbHkDpk9K1G"
      },
      "source": [
        "How does the run time of dynamic vs static scheduling compare when the program is run with multiple threads?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr3A62lh9bl1"
      },
      "source": [
        "We are using Python to assisst us with the speed up calculation below. \n",
        "\n",
        "Try calculate the speed-up for each of the (thread number, scheduling) combination. Replace the `??` in the Python code below with numbers from your table. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ffmt44Sh9trQ"
      },
      "outputs": [],
      "source": [
        "#lists holding measured times (floating point)\n",
        "#TODO: Fill in arrays below (code will not compile otherwise!)\n",
        "#Time (n threads)  1   2  3  4\n",
        "dd_static =       [ ??, ??, ??, ??]\n",
        "dd_dynamic=       [ ??, ??, ??, ??]\n",
        "\n",
        "#compute speedup\n",
        "static_speedup  = [round(dd_static[0]/dd_static[i],2)   for i in range(1,4)]\n",
        "dynamic_speedup = [round(dd_dynamic[0]/dd_dynamic[i],2) for i in range(1,4)]\n",
        "\n",
        "#print results\n",
        "print(\"static speedup:\")\n",
        "print(static_speedup)\n",
        "\n",
        "print(\"dynamic speedup:\")\n",
        "print(dynamic_speedup)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9-P-_lC-DRJ"
      },
      "source": [
        "## Summary - Static vs Dynamic Scheduling\n",
        "\n",
        "In many cases, static scheduling is sufficient. However, there is an implicit assumption with static scheduling that all components take about the same amount of time. However, if some components take longer than others, a **load balancing** issue can arise. In the case of the drug design example, different ligands take longer to compute than others. Therefore, a dynamic scheduling approach is better.\n",
        "\n",
        "Next, note that speedup is non-linear, especially at higher number of threads. While increasing the number of threads is supposed to generally _improve_ the run time of a program, it usually does not scale linearly with the number of cores. This occurs for a number of reasons. First, at higher number of threads, it is more likely that the serial components of a program (such as thread creation overhead, and any other necessary serial operations that must take place before the parallel code can run) start dominating run time (see **Amdahl’s Law**). Second, **resource contention** on the CPU or memory from other processes can cause slow downs. A related measure called **efficiency** measures how _well_ a program uses the cores assigned to it.\n",
        "\n",
        "Further Reading on Performance: [Dive into Systems, Chapter 14.4](https://diveintosystems.org/antora/diveintosystems/1.0/SharedMemory/performance.html)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Java OpenMP Patternlets",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}